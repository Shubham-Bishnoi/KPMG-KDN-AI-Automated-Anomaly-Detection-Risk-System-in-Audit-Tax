# -*- coding: utf-8 -*-
"""modelt3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tdgszk1x2-vPLovj0mzgAeVnvNhgWLHY

# 1. Bank cards analysis
"""

import polars as ps # type: ignore
import matplotlib.pyplot as plt # type: ignore
import numpy as np # type: ignore

df_cards = ps.read_csv('/content/cards_data.csv')

df_cards.head()

# Convert the 'credit_limit' column to numeric by removing dollar signs and commas
df_cards = df_cards.with_columns(
    ps.col('credit_limit')
    .str.replace(r'[\$,]', '', literal=False)  # Remove dollar signs and commas
    .cast(ps.Float64)  # Convert the cleaned string to a float
)

df_cards = df_cards.with_columns(ps.col("acct_open_date").str.strptime(ps.Date, format="%m/%Y").alias("acct_open_date"))

df_cards.head()

# Select string columns
string_columns = df_cards.select(ps.selectors.string()).columns

# Iterate over string columns and print the number of unique values
for col in string_columns:
    unique_count = df_cards[col].n_unique()
    print(f"{col}: {unique_count} unique values")

df_cards.filter(ps.col('card_on_dark_web') == 'Yes')

df_cards.drop_in_place('card_on_dark_web')

df_cards.head()

"""##Graphical Analysis

### 1. Histogram of Card Brands
  * Number of users for each card brand (Visa, Mastercard, etc.).
  * Subdivide the histogram by card_type (Credit, Debit, Prepaid).
"""

# Count occurrences of each card brand and card type correctly
card_counts = (
    df_cards
    .group_by(["card_brand", "card_type"])
    .agg(ps.len().alias("count"))
)

# Convert to lists for consistent ordering
brand_labels = sorted(card_counts["card_brand"].unique().to_list())
type_labels = sorted(card_counts["card_type"].unique().to_list())

# Create a dictionary to store counts for each (brand, type) pair
brand_type_counts = {brand: {card_type: 0 for card_type in type_labels} for brand in brand_labels}

for row in card_counts.iter_rows(named=True):
    brand_type_counts[row["card_brand"]][row["card_type"]] = row["count"]

# Convert to a NumPy array for stacked plotting
data_matrix = np.array([[brand_type_counts[brand].get(card_type, 0) for card_type in type_labels] for brand in brand_labels])

# Define colors for different card types
colors = plt.cm.Paired(np.linspace(0, 1, len(type_labels)))

# Plot the stacked bar chart
fig, ax = plt.subplots(figsize=(12, 6))
bottom = np.zeros(len(brand_labels))

bars = []
for i, card_type in enumerate(type_labels):
    bars.append(ax.bar(brand_labels, data_matrix[:, i], bottom=bottom, color=colors[i], edgecolor="black", label=card_type))
    bottom += data_matrix[:, i]  # Stack the bars

# Add labels and title
ax.set_xlabel("Card Brand", fontsize=12)
ax.set_ylabel("Count", fontsize=12)
ax.set_title("Distribution of Card Brands by Type", fontsize=14, fontweight="bold")
ax.legend(title="Card Type", loc="upper right")

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Annotate each stacked bar segment
for bar_group in bars:
    for bar in bar_group:
        height = bar.get_height()
        if height > 0:
            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2, int(height),
                    ha="center", va="center", fontsize=10, fontweight="bold", color="white")

# Show the plot
plt.show()

"""### 1.Bar Chart of Card Types

 * Number of users by card type (Credit, Debit, Prepaid).

"""

# Count occurrences of each card type
card_type_counts = (
    df_cards
    .group_by("card_type")
    .agg(ps.len().alias("count"))
    .sort("count", descending=True)
)

# Extract labels and values for plotting
card_types = card_type_counts["card_type"].to_list()
counts = card_type_counts["count"].to_list()

# Plot the bar chart
plt.figure(figsize=(8, 5))
plt.bar(card_types, counts, color="skyblue", edgecolor="black")

# Add labels and title
plt.xlabel("Card Type", fontsize=12)
plt.ylabel("Number of Users", fontsize=12)
plt.title("Number of Users by Card Type", fontsize=14, fontweight="bold")

# Annotate bars with counts
for i, count in enumerate(counts):
    plt.text(i, count + 0.1, str(count), ha="center", fontsize=12, fontweight="bold")

# Show the plot
plt.xticks(rotation=30)
plt.show()

"""### 1. Pie Chart of Card Brand Distribution

* Proportion of different card brands in the dataset.
"""

# Count occurrences of each card brand
card_brand_counts = (
    df_cards
    .group_by("card_brand")
    .agg(ps.len().alias("count"))
    .sort("count", descending=True)
)

# Extract labels and values for plotting
brands = card_brand_counts["card_brand"].to_list()
counts = card_brand_counts["count"].to_list()

# Define colors for better visualization
colors = plt.cm.Paired.colors[:len(brands)]

# Plot the pie chart
plt.figure(figsize=(8, 8))
plt.pie(counts, labels=brands, autopct="%1.1f%%", colors=colors, startangle=140, wedgeprops={'edgecolor': 'black'})

# Add title
plt.title("Proportion of Different Card Brands", fontsize=14, fontweight="bold")

# Show the plot
plt.show()

"""### 1. Histogram of Credit Limits
 * Distribution of credit_limit to see how limits vary across users.
"""

# Extract credit limit values
credit_limits = df_cards["credit_limit"].to_list()

# Plot the histogram
plt.figure(figsize=(10, 6))
plt.hist(credit_limits, bins=10, color="skyblue", edgecolor="black", alpha=0.7)

# Add labels and title
plt.xlabel("Credit Limit", fontsize=12)
plt.ylabel("Number of Users", fontsize=12)
plt.title("Distribution of Credit Limits", fontsize=14, fontweight="bold")

# Show the plot
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

"""### 1 . Box Plot of Credit Limits per Card Type
 * Compare credit limits between credit, debit, and prepaid cards.
"""

# Group data by card type
card_types = df_cards["card_type"].unique().to_list()
credit_limit_data = [df_cards.filter(df_cards["card_type"] == ct)["credit_limit"].to_list() for ct in card_types]

# Plot the box plot
plt.figure(figsize=(8, 6))
plt.boxplot(credit_limit_data, labels=card_types, patch_artist=True, boxprops=dict(facecolor="skyblue", alpha=0.7))

# Add labels and title
plt.xlabel("Card Type", fontsize=12)
plt.ylabel("Credit Limit", fontsize=12)
plt.title("Credit Limit Distribution by Card Type", fontsize=14, fontweight="bold")

# Show the plot
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

"""### 1. Box Plot of Credit Limits per Card Brand
 * Compare the credit limit distribution for Visa, Mastercard, etc.

"""

# Group data by card brand
card_brands = df_cards["card_brand"].unique().to_list()
credit_limit_data = [df_cards.filter(df_cards["card_brand"] == brand)["credit_limit"].to_list() for brand in card_brands]

# Plot the box plot
plt.figure(figsize=(8, 6))
plt.boxplot(credit_limit_data, labels=card_brands, patch_artist=True, boxprops=dict(facecolor="lightblue", alpha=0.7))

# Add labels and title
plt.xlabel("Card Brand", fontsize=12)
plt.ylabel("Credit Limit", fontsize=12)
plt.title("Credit Limit Distribution by Card Brand", fontsize=14, fontweight="bold")

# Show the plot
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

"""###1. Histogram of Number of Cards Issued
* Shows how many users have one or multiple cards.
"""

# Extract number of cards issued
num_cards = df_cards["num_cards_issued"].to_list()

# Plot the histogram
plt.figure(figsize=(8, 6))
plt.hist(num_cards, bins=range(1, max(num_cards) + 2), color="skyblue", edgecolor="black", alpha=0.7, align='left')

# Add labels and title
plt.xlabel("Number of Cards Issued", fontsize=12)
plt.ylabel("Number of Users", fontsize=12)
plt.title("Distribution of Number of Cards Issued", fontsize=14, fontweight="bold")

# Show the plot
plt.xticks(range(1, max(num_cards) + 1))  # Ensure ticks are at whole numbers
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

"""### 1.Time-Series Analysis of Account Opening Dates
 * Line chart showing the trend of account openings over time.
"""

yearly_counts = df_cards.group_by(
   df_cards["acct_open_date"].dt.year().alias("year")
).agg(
   ps.len().alias("num_accounts")
).sort("year")
# Extract data for plotting
years = yearly_counts["year"].to_list()
num_accounts = yearly_counts["num_accounts"].to_list()

# Plot the time series line chart
plt.figure(figsize=(10, 6))
plt.plot(years, num_accounts, marker="o", linestyle="-", color="royalblue", linewidth=2)

# Add labels and title
plt.xlabel("Year", fontsize=12)
plt.ylabel("Number of Accounts Opened", fontsize=12)
plt.title("Trend of Account Openings Over Time", fontsize=14, fontweight="bold")

# Show grid and plot
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.xticks(years, rotation=45)  # Ensure all years are labeled
plt.show()

"""### Numerical Analysis

 * Descriptive Statistics
"""

df_cards.describe()

"""# ****----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------****

# Statistic Analysis
"""



import requests

import pandas as pd

import json

import requests

import zipfile

import os

"""### 2. ExtracciÃ³n de datos
    2.1. Transactions_df
"""

# Cargar el dataset de transacciones directamente desde la URL
transactions_df = pd.read_csv("/content/transactions_data.csv")


# Extraer los `client_ids` Ãºnicos
client_ids = transactions_df['client_id'].unique()

# Display client_ids.
print(client_ids)

# Display Transactions_df.
transactions_df.head()

"""####2.2. Client Data y Card data"""

# Cargar los datos de los CSV
card_data = pd.read_csv("/content/cards_data.csv")

client_data = pd.read_csv("/content/users_data.csv")

# Display cards_df.

card_data.head()

# Display cards_df.

client_data.head()

# Rename id by client_id.

client_data.rename(columns={'id': 'client_id'}, inplace=True)

# Rename id by client_id.

client_data.rename(columns={'id': 'client_id'}, inplace=True)



# Rename id by card_data.

card_data.rename(columns={'id': 'card_id'}, inplace=True)



# Drop id from transactions_df and rename card_id to id.

transactions_df.rename(columns={'id': 'transactions_id'}, inplace=True)



# Comprueba si hay algun card_id que no este en cards_df.

transactions_df[~transactions_df['card_id'].isin(card_data['card_id'])]

# Merge transactions_df with card_data and client_data.

transactions_df_merged = transactions_df.merge(card_data, on='card_id', how='inner')

# Drop client_id_y from transactions_df_merged and rename client_id_x to client_id.

transactions_df_merged.drop(columns=['client_id_y'], inplace=True)

transactions_df_merged.rename(columns={'client_id_x': 'client_id'}, inplace=True)

# Merge transactions_df with client_data.

transactions_df_merged = transactions_df_merged.merge(client_data, on='client_id', how='inner')



# Display transactions_df.

transactions_df_merged.head()

#Set pandas to check all columns.

pd.set_option('display.max_columns', None)

# Display transactions_df_merged.

transactions_df_merged.head()

transactions_df_merged.to_csv("transactions_data_merged.csv", index=False)

transactions_df_merged[transactions_df_merged['client_id'] == 1556]

transactions_df_merged.dtypes

"""###3. Cleaning Formats"""

# Clean the variables amount, credit_limit, per_capita_income, yearly_income, total_debt removing the $ symbol and converting to float.

transactions_df_merged['amount'] = transactions_df_merged['amount'].str.replace('$', '').astype(float)

transactions_df_merged['credit_limit'] = transactions_df_merged['credit_limit'].str.replace('$', '').astype(float)

transactions_df_merged['per_capita_income'] = transactions_df_merged['per_capita_income'].str.replace('$', '').astype(float)

transactions_df_merged['yearly_income'] = transactions_df_merged['yearly_income'].str.replace('$', '').astype(float)

transactions_df_merged['total_debt'] = transactions_df_merged['total_debt'].str.replace('$', '').astype(float)

# Convert the variables expire and acct_open_date to datetiem format as MM/YYYY.

transactions_df_merged['expires'] = pd.to_datetime(transactions_df_merged['expires'], format='%m/%Y')

transactions_df_merged['acct_open_date'] = pd.to_datetime(transactions_df_merged['acct_open_date'], format='%m/%Y')

# Export the cleaned dataset to a CSV file.

transactions_df_merged.to_csv("transactions_df_merged.csv", index=False)

"""****------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------****

#Financial transactions
"""

import numpy as np
import pandas as pd

import folium #For maps
from folium.plugins import FeatureGroupSubGroup

import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('ggplot')


import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings('ignore')

users_data = pd.read_csv('/content/users_data.csv')
transactions_data = pd.read_csv('/content/transactions_data.csv')
cards_data = pd.read_csv('/content/cards_data.csv')

"""##EDA On Users Data"""

users_data.head(5)

users_data.dtypes

users_data['per_capita_income'] = users_data['per_capita_income'].replace({'\$': ''}, regex = True).astype(int)
users_data['yearly_income'] = users_data['yearly_income'].replace({'\$': ''}, regex = True).astype(int)
users_data['total_debt'] = users_data['total_debt'].replace({'\$': ''}, regex = True).astype(int)

users_data.isnull().sum()

users_data.duplicated().sum()

sns.histplot(data = users_data, x = 'current_age', hue = 'gender', multiple = 'stack', kde = True)

sns.histplot(data = users_data, x = 'credit_score', hue = 'gender', multiple = 'stack', kde = True)

sns.histplot(data = users_data, x = 'num_credit_cards', hue = 'gender', multiple = 'stack')

sns.histplot(data = users_data, x = 'yearly_income', hue = 'gender', multiple = 'stack')

sns.histplot(data = users_data, x = 'total_debt', hue = 'gender', multiple = 'stack')

sns.scatterplot(data = users_data, x = 'yearly_income', y = 'total_debt', hue = 'num_credit_cards', alpha = 0.5)

fig = plt.figure(figsize=(15, 15))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(users_data['yearly_income'], users_data['total_debt'], users_data['credit_score'], c='red', marker='o', alpha = 0.3)

ax.set_xlabel('Yearly Income')
ax.set_ylabel('Total Debt')
ax.set_zlabel('Credit Score')

plt.show()

m = folium.Map(location=[users_data["latitude"].mean(), users_data["longitude"].mean()], zoom_start=5)

fg_main = folium.FeatureGroup(name="Income Categories").add_to(m)
fg_above = FeatureGroupSubGroup(fg_main, "Above Per Capita Income").add_to(m)
fg_below = FeatureGroupSubGroup(fg_main, "Below Per Capita Income").add_to(m)
fg_equal = FeatureGroupSubGroup(fg_main, "Equal to Per Capita Income").add_to(m)

for _, row in users_data.iterrows():
    if row["yearly_income"] > row["per_capita_income"]:
        color, group = "green", fg_above
    elif row["yearly_income"] < row["per_capita_income"]:
        color, group = "red", fg_below
    else:
        color, group = "blue", fg_equal

    folium.CircleMarker(
        location=[row["latitude"], row["longitude"]],
        radius=5,
        color=color,
        fill=True,
        fill_color=color,
        fill_opacity=0.2,
        popup=f"Income: ${row['yearly_income']}<br>Per Capita: ${row['per_capita_income']}"
    ).add_to(group)

folium.LayerControl(collapsed=False).add_to(m)

m

"""###EDA on Cards Data"""

cards_data.head(5)

cards_data.isnull().sum()

cards_data.duplicated().sum()

cards_data.dtypes

cards_data['credit_limit'] = cards_data['credit_limit'].replace({'\$': ''}, regex = True).astype(int)

sns.histplot(data = cards_data, x = 'card_type', hue = 'card_brand', multiple = 'dodge', palette = sns.color_palette("viridis"))

temp = cards_data.groupby('client_id').agg(total_credit_limit = ('credit_limit', 'sum'), total_cards = ('id', 'count'))
temp

plt.figure(figsize = (15,7))
sns.histplot(data = temp, x  = 'total_credit_limit', hue = 'total_cards', multiple = 'stack')

cards_data['card_on_dark_web'].value_counts()

"""###EDA on Transactions Data



"""

transactions_data.head(5)

transactions_data.isnull().sum()

transactions_data.duplicated().sum()

transactions_data['errors'].value_counts()

transactions_data['amount'] = transactions_data['amount'].replace({'\$': ''}, regex = True).astype(float)

#plt.figure(figsize = (9, 7))
sns.histplot(data = transactions_data, x = 'amount')
plt.xscale('symlog')

transactions_data['merchant_id'].value_counts()

transactions_data['use_chip'].value_counts()

transactions_data['mcc'].value_counts()

# Convert the 'date' column to datetime.
# Using errors='coerce' and infer_datetime_format=True allows pandas to handle mixed formats.
transactions_data['date'] = pd.to_datetime(
    transactions_data['date'],
    errors='coerce',
    infer_datetime_format=True
)

# Optional: Check how many dates failed to convert.
print("Number of unparsed dates:", transactions_data['date'].isna().sum())

# Create separate columns for the date and time parts.
transactions_data['date_part'] = transactions_data['date'].dt.date
transactions_data['time_part'] = transactions_data['date'].dt.time

# Display the first 5 rows after processing the date.
print(transactions_data.head(5))

transactions_data['date'].dt.year.value_counts()

transactions_data.groupby('date_part', as_index = False)['amount'].sum()

transactions_data['time_part'] = transactions_data['date'].dt.strftime('%H:%M:%S')
transactions_data.groupby('time_part', as_index = False)['amount'].sum()

users_data.shape, transactions_data.shape, cards_data.shape

users_data['num_credit_cards'].sum()

collective_users_data = pd.merge(cards_data, users_data, left_on='client_id', right_on='id', how='inner')

collective_users_data.shape

collective_users_data.head(7)

collective_users_data.columns

collective_users_data.rename(columns = {'id_x': 'card_id'}, inplace = True)

collective_users_data.drop(columns = ['id_y'], inplace = True)

all_data = pd.merge(transactions_data, collective_users_data, on = ['client_id', 'card_id'])

all_data.shape

all_data.head(7)

all_data.columns

"""##Task

#ðŸ“ˆ Exercises

##ðŸŒ Background

In today's digital economy, banks and financial institutions process massive amounts of transactional data daily, which includes purchases, payments, transfers, and other activities. This data is not only pivotal for monitoring individual financial behavior but also critical for detecting fraudulent transactions, understanding customer spending patterns, and optimizing personalized financial services. The goal of this challenge is to leverage transaction data from various sources, such as APIs and pre-existing datasetsâ€”merge and process it for meaningful analysis, and train machine learning models that can predict outcomes like fraud detection, credit scoring, or customer segmentation. In the final step, an AI agent will be developed to combine these analytical and predictive processes, offering an automated and intelligent solution for decision-making and insights generation.


###ðŸ“Š Data Processing

To effectively analyze and model the data, you will need to implement the following steps:

API Integration: First, you'll need to implement API calls to fetch all necessary data from the given endpoints, including customer information and card data.

Data Merging and Cleaning: After retrieving the data, merge the customer, card, and transaction datasets using relevant keys (such as client_id or card_id). Ensure that the data is cleaned and preprocessed to handle any missing or inconsistent values.

Data Transformation: Apply further transformations, such as creating new features, aggregating data (e.g., monthly expenses), and encoding categorical variables. These preprocessing steps are crucial to preparing the data for machine learning models.

###ðŸ¤– Model

For the modeling phase, you are tasked with developing two different machine learning models, each serving a distinct purpose:

Fraud Detection Model: This model will classify transactions as either fraudulent or non-fraudulent. The goal is to identify suspicious transactions in real-time by analyzing patterns such as transaction frequency, amounts, and locations. Supervised learning techniques, such as logistic regression, decision trees, or
ensemble methods, can be employed. Since fraud is often rare, handling class imbalance effectively (e.g., through resampling or custom loss functions) will be key to building a robust model.

Expenses Forecast Model: This model will forecast future customer expenses based on their historical spending patterns. Time series forecasting models can be used to predict future expenditures, helping banks provide personalized budget management tools. The model will learn from past transaction data and predict the monthly expenses over the next three months from each client's last transaction.

###ðŸŽ¯ Tasks:
The challenge will be divided in three subparts: Data Statistics, Data Prediction and IA Agent creation. For each part you have a set of tasks. You have examples of the expected submission for tasks 1, 3 and 4 in the /predictions folder.

* Part 1:

    * Task 1: Submit the answers to the following queries:

          query_1: The card_id with the latest expiry date and the lowest credit limit amount.

          query_2: The client_id that will retire within a year that has the lowest credit score and highest debt.

          query_3: The transaction_id of an Online purchase on a 31st of December with the highest absolute amount (either earnings or expenses).

          query_4: Which client over the age of 40 made the most transactions with a  Visa card in February 2016? Please return the client_id, the card_id involved, and the total number of transactions.

     * Task 2: Implement the functions stated on the file src/data/data_functions.py. The input dataset will be the transactions.csv file in its given raw format with the dates parsed into datetime format. The expected functionality of each function is described within the src/data/data_functions.py file:

            Function: earnings_and_expenses(data,client_id,start_date,end_date)
            Function: expenses_summary(data,client_id,start_date,end_date)
            Function: cash_flow_summary(data,client_id,start_date,end_date)

* Part 2:

     * Task 3: Create a fraud detection model capable of detecting fraudulent
transactions. The dataset transactions_data.csv will be split for training and prediction. Predictions should only be made for the transaction IDs listed in predictions/predictions_3.json.

     * Task 4: Develop a forecast model to predict monthly expenses (negative amounts) for each client in predictions/predictions_4.json. Forecast the monthly expenses for the next three months based on each client's most recent transaction date.



* Part 3:

    * Task 5: In this task, your objective is to implement a simple AI Agent with Langchain and the Language Model llama3.2:1b, hosted locally via Ollama. The Agent should be able to process a natural language input and generate a summary report in PDF format by using the functions developed in Task 2. The input of the functions run_agent() will include the client_id , df:pd.DataFrame and the text prompt. The Agent must extract the relevant start and end dates from the prompt and be able to invoke the three functions to generate the report. You can find this function and its definition in src/agent/agent.py.
          Take into account the non-deterministic nature of language models, as this model is a Small Language Model, use the adequate prompt engineering techniques to ensure the optimal results.
"""

#imports for Bar Plot
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pandas as pd

def earnings_and_expenses(
    df: pd.DataFrame, client_id: int, start_date: str, end_date: str
) -> pd.DataFrame:
    #reading input dataframe
    transactions_data = pd.read_csv(df)
    #sort values by client_id and date
    transactions_data.sort_values(by=['client_id', 'date'], inplace=True)
    #changing amount datatype to float
    transactions_data['amount'] = transactions_data['amount'].str.replace('$', '').astype(float)
    #client_data for the specified id and date range inclusive
    client_data = transactions_data[(transactions_data['client_id'] == client_id) & (transactions_data['date'] >= start_date) & (transactions_data['date'] <= end_date) & transactions_data['errors'].isna()]
    #earnings and expenses to create a DF
    earnings = client_data[client_data['amount'] > 0]['amount'].round(2)
    expenses = client_data[client_data['amount'] < 0]['amount'].round(2)
    earn_expend = pd.DataFrame({'Earnings': earnings, 'Expenses': expenses})
    #create bar plot with the Earnings and Expenses absolute values
    sns.barplot(data=earn_expend.abs())
    plt.savefig('reports/figures/earnings_and_expenses.png')
    return earn_expend
    """
    For the period defined in between start_date and end_date (both included), get the client data available and return
    a pandas DataFrame with the Earnings and Expenses total amount for the period range and user given. The expected columns are:
        - Earnings
        - Expenses
    The DataFrame should have the columns in this order ['Earnings', 'Expenses']. Round the amounts to 2 decimals.

    Create a Bar Plot with the Earnings and Expenses absolute values and save it as "reports/figures/earnings_and_expenses.png".

    Parameters
    ----------
    df : pandas DataFrame
       DataFrame of the data to be used for the agent.
    client_id : int
        Id of the client.
    start_date : str
        Start date for the date period. In the format "YYYY-MM-DD".
    end_date : str
        End date for the date period. In the format "YYYY-MM-DD".

    Returns
    -------
    Pandas Dataframe with the earnings and expenses rounded to 2 decimals.
    """






def expenses_summary(
    df: pd.DataFrame, client_id: int, start_date: str, end_date: str
) -> pd.DataFrame:
    #reading input dataframe
    transactions_data = pd.read_csv(df)
    #sort values by client_id and date
    transactions_data.sort_values(by=['client_id', 'date'], inplace=True)
    #changing amount datatype to float
    transactions_data['amount'] = transactions_data['amount'].str.replace('$', '').astype(float)
    #loading mcc codes json file
    with open(f'/kaggle/input/transactions-fraud-datasets/mcc_codes.json', 'r') as f:
        mcc_codes = json.load(f)
    mcc_codes_data = pd.DataFrame.from_dict(mcc_codes, orient='index', columns=['mcc_description'])
    #naming the index column for adding the mcc_description column to the transactions_data
    mcc_codes_data.index.name = 'mcc_code'
    mcc_codes_data.reset_index(inplace=True)
    #changing datatype of mcc_code from object to int
    mcc_codes_data['mcc_code'] = mcc_codes_data['mcc_code'].astype(int)
    mcc_codes_dict = mcc_codes_data.set_index('mcc_code')['mcc_description'].to_dict()
    #mapping the codes from mcc_codes_data to transactions_data
    transactions_data['merchant_category_name'] = transactions_data['mcc'].map(mcc_codes_dict)
    #client_data for the specified id and date range inclusive
    client_data = transactions_data[(transactions_data['client_id'] == client_id) & (transactions_data['date'] >= start_date) & (transactions_data['date'] <= end_date) & transactions_data['errors'].isna()]
    #renaming merchant category name
    client_data.rename(columns={'merchant_category_name':'Expenses Type'}, inplace=True)
    #usage of aggregation to find the expenses summary
    merchant_category_data = client_data.groupby('merchant_category_name').agg({'amount': ['sum', 'count', 'mean', 'max', 'min']})
    #renaming column names
    merchant_category_data.rename(columns={'sum': 'Total Amount', 'count': 'Num. Transactions', 'mean': 'Average', 'max': 'Max', 'min': 'Min'}, inplace=True)
    #sorting values by merchant category name
    merchant_category_data.sort_values(by='Expenses Type', inplace=True)
    #rounding up to two decimals
    merchant_category_data.round(2)
    #create bar plot with the Expenses Summary values
    sns.barplot(data=merchant_category_data['amount'].abs())
    plt.savefig("reports/figures/expenses_summary.png")
    return merchant_category_data
    """
    For the period defined in between start_date and end_date (both included), get the client data available and return
    a Pandas Data Frame with the Expenses by Merchant Category. The expected columns are:
        - Expenses Type --> (merchant category names)
        - Total Amount
        - Average
        - Max
        - Min
        - Num. Transactions
    The DataFrame should be sorted alphabetically by Expenses Type and values have to be rounded to 2 decimals.
    Return the dataframe with the columns in the given order.
    The merchant category names can be found in data/raw/mcc_codes.json.

    Create a Bar Plot with the data in absolute values and save it as "reports/figures/expenses_summary.png".

    Parameters
    ----------
    df : pandas DataFrame
       DataFrame of the data to be used for the agent.
    client_id : int
        Id of the client.
    start_date : str
        Start date for the date period. In the format "YYYY-MM-DD".
    end_date : str
        End date for the date period. In the format "YYYY-MM-DD".

    Returns
    -------
    Pandas DataFrame with the Expenses by merchant category.
    """




def cash_flow_summary(
    df: pd.DataFrame, client_id: int, start_date: str, end_date: str
) -> pd.DataFrame:
    #reading input dataframe
    transactions_data = pd.read_csv(df)
    #sort values by client_id and date
    transactions_data.sort_values(by=['client_id', 'date'], inplace=True)
    #changing amount datatype to float
    transactions_data['amount'] = transactions_data['amount'].str.replace('$', '').astype(float)
    #client_data for the specified id and date range inclusive
    client_data = transactions_data[(transactions_data['client_id'] == client_id) & (transactions_data['date'] >= start_date) & (transactions_data['date'] <= end_date) & transactions_data['errors'].isna()]
    #creating a column Date with Month or Week segmentation.
    if (end_date - start_date).days > 60:
        client_data['Date'] = client_data['date'].dt.to_period('M').dt.to_timestamp('M')
    else:
        client_data['Date'] = client_data['date'].dt.to_period('W').dt.to_timestamp('W')
    #creating cash flow summary dataframe
    cash_flow_summary = client_data.groupby('Date').apply(lambda x: pd.Series({'inflows': x[x['amount'] > 0]['amount'].sum().round(2), 'outflows': x[x['amount'] < 0]['amount'].abs().sum().round(2), 'netcashflow': x[x['amount'] > 0]['amount'].sum()-x[x['amount'] < 0]['amount'].abs().sum().round(2), 'savings_percent': 100*((x[x['amount'] > 0]['amount'].sum()-x[x['amount'] < 0]['amount'].abs().sum())/(x[x['amount'] > 0]['amount'].sum())).round(2)}))
    return cash_flow_summary
    """
    For the period defined by start_date and end_date (both inclusive), retrieve the available client data and return a Pandas DataFrame containing cash flow information.

    If the period exceeds 60 days, group the data by month, using the end of each month for the date. If the period is 60 days or shorter, group the data by week.

        The expected columns are:
            - Date --> the date for the period. YYYY-MM if period larger than 60 days, YYYY-MM-DD otherwise.
            - Inflows --> the sum of the earnings (positive amounts)
            - Outflows --> the sum of the expenses (absolute values of the negative amounts)
            - Net Cash Flow --> Inflows - Outflows
            - % Savings --> Percentage of Net Cash Flow / Inflows

        The DataFrame should be sorted by ascending date and values rounded to 2 decimals. The columns should be in the given order.

        Parameters
        ----------
        df : pandas DataFrame
           DataFrame  of the data to be used for the agent.
        client_id : int
            Id of the client.
        start_date : str
            Start date for the date period. In the format "YYYY-MM-DD".
        end_date : str
            End date for the date period. In the format "YYYY-MM-DD".


        Returns
        -------
        Pandas Dataframe with the cash flow summary.

    """

"""# Transaction Analysis and Prediction"""

# Importing the necessary libraries for analysis
import numpy as np
import pandas as pd
import json

# Data Visualization libraries
import altair as alt
import matplotlib.pyplot as plt
import seaborn as sns

#Imporing libraries to avoid warnings
import warnings

# Applying the important settings
pd.set_option('display.max_columns', None)
warnings.filterwarnings('ignore')

"""## Importing the data
### Reading the CSV files
"""

# Importing datasets
card = pd.read_csv('/content/cards_data.csv')
trans = pd.read_csv('/content/transactions_data.csv')
users = pd.read_csv('/content/users_data.csv')

"""### Reading and Manipulating the JSON files"""

#Reading the JSON files
with open('/content/mcc_codes.json', 'r') as file:
    codes = json.load(file)

with open('/content/train_fraud_labels.json', 'r') as file:
    fraud = json.load(file)

fraud_id = list(fraud['target'].keys())
fraud_status = list(fraud['target'].values())

df1 = pd.DataFrame({"id": fraud_id, 'Status': fraud_status})
df1.head(3)

code = list(codes.keys())
name = list(codes.values())

df2 = pd.DataFrame({'mcc': code, 'Name': name})
df2.head(3)

"""## Making the datasets ready for analysis

### Cards data
"""

card.head(3)

card.shape

# Checking for any duplicated or missing values
print("Total Duplicated values found: ", card.duplicated().sum())

print("Count of null values in each column are as follows: \n", card.isnull().sum())

# Checking if correct datatypes are assigned to the columns

nuuniq = []
dtyp = []
uuniq = []

for col in card.columns:
    nuuniq.append(card[col].nunique())
    dtyp.append(card[col].dtype)
    uuniq.append(card[col].unique()[0])

result = pd.DataFrame({"Column Name": card.columns, 'Count of Unique values': nuuniq, 'Assigned Datatype': dtyp, 'Value': uuniq})
result

"""### Considerations

   * The column 'card_on_dark_web' has a constant value.

### Issues Found:

1.  The column 'Expires' and 'acct_open_date' is misassiged as object and should be datetime.
2.  'credit_limit' column is misassigned as object due to the presence of '$' symbol. It should be numeric.
"""

# Fixing the issues found above.

# Converting the relevant columns to datetime
card['expires'] =pd.to_datetime(card['expires'])
card['acct_open_date'] = pd.to_datetime(card['acct_open_date'])

# Fixing the datatype misassignment for 'credit_limit' column
card['credit_limit'] = card['credit_limit'].str.replace('$', '')
card['credit_limit'] = pd.to_numeric(card['credit_limit'])
card.rename(columns = {'credit_limit': 'Credit_Limit($)'})

# Rechecking the datatypes
nuuniq = []
dtyp = []
uuniq = []

for col in card.columns:
    nuuniq.append(card[col].nunique())
    dtyp.append(card[col].dtype)
    uuniq.append(card[col].unique()[0])

result = pd.DataFrame({"Column Name": card.columns, 'Count of Unique values': nuuniq, 'Assigned Datatype': dtyp, 'Value': uuniq})
result

"""### Users data"""

users.head(3)

"""* Problem 1: A study of the dataset reveals that one can work on customer segmentation to help understand different types of customers.

* Problem 2: When users data is merged with card data, one can work on the regression problem of predicting credit limit for a customer based on the user's data.
"""

# Checking if correct datatypes are assigned to the columns

nuuniq = []
dtyp = []
uuniq = []

for col in users.columns:
    nuuniq.append(users[col].nunique())
    dtyp.append(users[col].dtype)
    uuniq.append(users[col].unique()[0])

result = pd.DataFrame({"Column Name": users.columns, 'Count of Unique values': nuuniq, 'Assigned Datatype': dtyp, 'Value': uuniq})
result

"""* The columns 'per_capita_income', 'yearly_income' and 'total_debt' columns are assigned as object due to the presence of '$'. Resolving the issue and changing the datatype to numeric."""

# Removing the '$' sign from the columns
users['per_capita_income'] = users['per_capita_income'].str.replace('$', '')
users['yearly_income'] = users['yearly_income'].str.replace('$', '')
users['total_debt'] = users['total_debt'].str.replace('$', '')

# Changing their datatype to numeric
users['per_capita_income'] = pd.to_numeric(users['per_capita_income'])
users['yearly_income'] = pd.to_numeric(users['yearly_income'])
users['total_debt'] = pd.to_numeric(users['total_debt'])

# Changning the names
users.rename(columns = {"per_capita_income": "per_capita_income($)", "yearly_income" : "yearly_income($)", "total_debt": "total_debt($)"}, inplace = True)

# Verifying the changes
users.dtypes

# Checking for any null values in the data

users.isnull().sum()

dfr = pd.merge(users, card, on = 'id', how = 'inner')
dfr.head(3)

"""### Codes data"""

df2.head(3)

# Checking the datatype of the column on which the merger will take place
df2['mcc'].dtype

df2['mcc'] = pd.to_numeric(df2['mcc'])


# Checking for any missing values
df2.isnull().sum()

"""### Fraud data"""

df1.shape

df1.isnull().sum()

# Understanding the data types
df1.dtypes

df1['id'] = pd.to_numeric(df1['id'])

"""### Transaction data"""

trans.head(3)

# Understanding the datatype of the common column.
trans['id'].dtype

"""* Merging the fraud data with the transaction data"""

df = pd.merge(trans, df1, on='id', how='inner')
df.head(3)

"""* Merging the MCC code data with the new data"""

df = pd.merge(df, df2, on = 'mcc', how = 'inner')
df.head(3)

"""* Problem 3: The above-merged data can be used for classifying fraudulent transactions.

## Data Cleaning

* 1. Checking and validating the datatypes
"""

nuuniq = []
dtyp = []
uuniq = []

for col in df.columns:
    nuuniq.append(df[col].nunique())
    dtyp.append(df[col].dtype)
    uuniq.append(df[col].unique()[0])

result = pd.DataFrame({"Column Name": df.columns, 'Count of Unique values': nuuniq, 'Assigned Datatype': dtyp, 'Value': uuniq})
result

"""### Observations:

* The 'Amount' column is wrong wrongly assigned as 'Object' due to the presence of '$' in the value.
"""

df['amount'] = df['amount'].str.replace('$', '')
df['amount'] = pd.to_numeric(df['amount'])
df.rename(columns = {'amount': 'Amount($)'}, inplace = True)

"""* Issues found:

      Presence of negative values in the account column
      Presence of NAN values in 'errors' column


### 2. Dealing with missing and improper values

* 2.1 Missing values

          As the dataset is large enough it isn't easy to get an idea of missing values. Hence using percentages instead.
"""

miss = []
rows = df.shape[0]
per = []

for col in df.columns:
    missing = df[col].isnull().sum()
    miss.append(missing)
    percent = ( missing / rows)*100
    per.append(percent)

res = pd.DataFrame({'Column Name': df.columns, 'Missing Values': miss, 'Percentage of null values': per})
res

"""* Dealing with Error column"""

df['errors'].unique()

df['errors'].fillna('Errorless', inplace = True)

"""* Dealing with Merchant State columns

       Filling missing values in the merchant_state column with the mode ensures consistency by using the most common value, which likely represents the majority location and minimizes disruption to categorical distributions.


"""

df['merchant_state'].fillna(df['merchant_state'].mode()[0], inplace = True)

"""* Dealing with zip column

         Studying the box-plot of the 'zip' column to identify the proper imputation technique


"""

df['zip'].plot(kind = 'box')

df['zip'].fillna(df['zip'].mean(), inplace = True)

df.isnull().sum()

"""### 2.2. Handling invalid values

* The 'Amount' column is suspected to have some invalid values.
"""

amt = df['Amount($)'].sort_values(ascending = True)
amt

amt_below = df[df['Amount($)'] <= 0]
amt_below.shape

"""* The rationale for interpreting the Amount column with both positive and negative values as representing credits and debits to a consumer's account is grounded in standard financial transaction conventions

       1. Logical Consistency: Positive values are typically associated with deposits or credits, while negative values indicate debits or withdrawals. This assumption aligns with standard accounting practices, making the data more intuitive for analysis.

       2. Enhanced Interpretability: Treating positive amounts as credits and negative amounts as debits allows for straightforward interpretations and calculations of net balances, transaction patterns, and consumer behavior.

       3. Simplified Analysis and Modeling: By defining these value signs as credit and debit indicators, we streamline analytical tasks such as identifying high-expenditure consumers or tracking cash flow patterns, both of which are essential for generating actionable insights.

* This assumption provides a practical framework for interpreting and working with the Amount data in a way that enhances analytical depth and maintains alignment with common financial practices.
"""

amount = df['Amount($)']
payment_status = []

for val in amount:
    if val < 0:
        payment_status.append('Debit')
    else:
        payment_status.append('Credit')

df['Payment_Type'] = payment_status

df.columns

df.rename(columns = {"id": "ID", 'date': 'Date', 'client_id': 'CID', 'Amount($)': 'Amount', 'use_chip': 'UseChip', 'merchant_id': 'MID', 'merchant_city': 'MCity', 'merchant_state': 'MState', 'zip': 'Pincode', 'mcc': 'MCC', 'errors': 'Error', 'Status': 'Fraud_Status', 'Name': 'Category'}, inplace = True)

df.head(3)

"""### Analysis on classification dataset
* Fraud Status column
"""

per = df['Fraud_Status'].value_counts(normalize = True)*100

legit = per[0]
fraud = per[1]

percentages = [legit, fraud]
labels = ['Legitimate', 'Fraud']

# Bar chart
plt.bar(labels, percentages, color = ['#1e8449', '#cb4335'])
plt.title('Percentage of Transactions by Fraud Status')
plt.xlabel('Transaction type')
plt.ylabel('Percentage')

for i, v in enumerate(percentages):
  plt.text(i, v + 1, str(round(v, 2)) + '%', ha='center')

plt.show()

"""### Interpretation:

     One can see major imbalance in the fraud status column.

### Considerations:

     If a classification task is to be performed then it is important to ensure balanced learning for the model, considering the imbalance a hybrid strategy of undersampling of the majority class and oversampling of the minority class seems like a god option to consider.

### Amount Column
"""

# Statistical Distribution of the Amount variable
print('Percentile Values')
print(df['Amount'].quantile([0.0, 0.25, 0.5, 0.75, 1]))

# Removing the negative sign as the amount cannot be negative
#To handle the positives and negatives the 'Payment_Status' column has been introduced.

df['Amount'] = df['Amount'].astype('str')
df['Amount'] = df['Amount'].str.replace('-', '')
df['Amount'] =pd.to_numeric(df['Amount'])

# Statistical Distribution of the entire Amount variable without any negative signs
print('Percentile Values')
print(df['Amount'].quantile([0.0, 0.25, 0.5, 0.75, 1]))

amt = df[df['Amount'] > 71.00 ]
amt['Fraud_Status'].value_counts()

# INVESTIGATING FURTHER TO UNDERSTAND IF THERE IS A CERTAIN RANGE OF AMOUNT FOR FRAUDULENT TRANSACTIONS

amt_fraud = amt[amt['Fraud_Status'] == 'Yes']
amt_legit = amt[amt['Fraud_Status'] == 'No']

# Summary Statistics for the last 25th percentile (Which are fraud)
print('Fraud transaction summary stats:')
print("Mean: ", amt_fraud['Amount'].mean())
amt_fraud['Amount'].quantile([0, 0.25, 0.5, 0.75, 1])

# Summary Statistics for the last 25th percentile (Which are legitimate)
print('Legitimate transactions summmary stats:')
print('Mean: ', amt_legit['Amount'].mean())
amt_legit['Amount'].quantile([0, 0.25, 0.5, 0.75, 1])

"""* There can be seen a slight difference between fraud and legitimate transactions, the fraud transactions show a high range of Amount values when compared to the legitimate ones.



* Summary of the insights found from the 'Amount' column:

           The analysis of the 'Amount' column highlighted that values in the lower 25th percentile, initially negative (indicating credits above 100), were corrected by removing the negative sign since transaction amounts cannot be negative. To clearly distinguish between credit and debit transactions, a new 'Payment_Status' column was introduced. Upon analyzing the adjusted 'Amount' column, the distribution indicated a similar overall pattern but suggested a need for deeper investigation into the last 25th percentile for potential fraudulent activity. Further exploration revealed that fraudulent transactions tend to have higher 'Amount' values compared to legitimate ones within this range. This finding suggests a potential link between higher transaction amounts in this percentile and fraudulent activity



### Category column
"""

fraud = df[df['Fraud_Status'] == 'Yes']
legit = df[df['Fraud_Status'] == 'No']

print("Total categories present in the data: ", df['Category'].nunique())
print("Categories involved in fraud transactions: ", fraud['Category'].nunique())

df['Category'].value_counts().plot(kind = 'bar')
plt.title('Transactions by category')
plt.show()

"""It can be inferred from the above visualization that there are some categories which have a lot of transactions and some have very less transactions showing non-uniform distribution."""

df['Category'].value_counts().head(20).plot(kind = 'bar')
plt.title('Top 20 categories having the maximum transactions')
plt.show()

"""Grocery stores, supermarkets', and 'Miscellaneous Food Stores' are the categories having the maximum number of transactions. Investigating the fraudulent transactions would help us understand and identify trends for fraudulent transactions.


"""

fraud['Category'].value_counts().plot(kind = 'bar')
plt.title('Fraudulent transactions by category')
plt.show()

"""Different categories can be seen at which top of the table for fraudulent transactions. Investigating the top legitimate transaction categories might determine the difference between the two transaction types."""

legit['Category'].value_counts().sort_values(ascending = False).head(20).plot(kind = 'bar')
plt.title('Legitimate transactions by categories')

"""The top categories for legitimate transactions and fraudulent transactions are different which state that the 'Categorical' has a certain predictive power towards classifying transactions as fraud and legitimate.

# ****----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------****

# Prediction Model

* The goal of this project is to train a machine learning model from transaction data managed via SQL. The model will predict customer churn based on transaction and demographic data from computingvictor's Financial Transactions Dataset

* There are three main datasets I will use:

      Transactions (transactions_data.csv): Contains transaction details to infer spending habits.
      Cards (cards_dat.csv): Provides card-related attributes like type and limit.
      Users (users_data.csv): Demographic details that can help understand customer profiles.

Due to the size of the datasets involved, I will mainly manage data using BigQuery, with dynamic SQL querying using R. I will also restrict our analysis to the top 10 merchants (by revenue) in the transactions data set.

## Table of Contents

* Introduction
* Preparations
* What's Churn?
* Feature Engineering
* Preparing Data for ML
* Training and Testing ranger Model
* Model Evaluation
* Conclusion
"""





